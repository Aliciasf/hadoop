Hadoop Change Log


Trunk (unreleased)

 1. Fix HADOOP-126. 'bin/hadoop dfs -cp' now correctly copies .crc
    files.  (Konstantin Shvachko via cutting)

 2. Fix HADOOP-51. Change DFS to support per-file replication counts.
    (Konstantin Shvachko via cutting)

 3. Fix HADOOP-131.  Add scripts to start/stop dfs and mapred daemons.
    Use these in start/stop-all scripts.  (Chris Mattmann via cutting)

 4. Stop using ssh options by default that are not yet in widely used
    versions of ssh.  Folks can still enable their use by uncommenting
    a line in conf/hadoop-env.sh. (cutting)

 5. Fix HADOOP-92.  Show information about all attempts to run each
    task in the web ui.  (Mahadev konar via cutting)

 6. Fix HADOOP-128.  Improved DFS error handling. (Owen O'Malley via cutting)

 7. Fix HADOOP-129.  Replace uses of java.io.File with new class named
    Path.  This fixes bugs where java.io.File methods were called
    directly when FileSystem methods were desired, and reduces the
    likelihood of such bugs in the future.  It also makes the handling
    of pathnames more consistent between local and dfs FileSystems and
    between Windows and Unix. java.io.File-based methods are still
    available for back-compatibility, but are deprecated and will be
    removed once 0.2 is released. (cutting)

 8. Change dfs.data.dir and mapred.local.dir to be comma-separated
    lists of directories, no longer be space-separated. This fixes
    several bugs on Windows. (cutting)

 9. Fix HADOOP-144.  Use mapred task id for dfs client id, to
    facilitate debugging.  (omalley via cutting)

10. Fix HADOOP-143.  Do not line-wrap stack-traces in web ui.
    (omalley via cutting)

11. Fix HADOOP-118.  In DFS, improve clean up of abandoned file
    creations.  (omalley via cutting)

12. Fix HADOOP-138.  Stop multiple tasks in a single heartbeat, rather
    than one per heartbeat.  (Stefan via cutting)

13. Fix HADOOP-139.  Remove a potential deadlock in
    LocalFileSystem.lock().  (Igor Bolotin via cutting)

14. Fix HADOOP-134.  Don't hang jobs when the tasktracker is
    misconfigured to use an un-writable local directory.  (omalley via cutting)

15. Fix HADOOP-115.  Correct an error message.  (Stack via cutting)

16. Fix HADOOP-133.  Retry pings from child to parent, in case of
    (local) communcation problems.  Also log exit status, so that one
    can distinguish patricide from other deaths.  (omalley via cutting)

17. Fix HADOOP-142.  Avoid re-running a task on a host where it has
    previously failed.  (omalley via cutting)

18. Fix HADOOP-148.  Maintain a task failure count for each
    tasktracker and display it in the web ui.  (omalley via cutting)

19. Fix HADOOP-151.  Close a potential socket leak, where new IPC
    connection pools were created per configuration instance that RPCs
    use.  Now a global RPC connection pool is used again, as
    originally intended.  (cutting)

20. Fix HADOOP-69.  Don't throw a NullPointerException when getting
    hints for non-existing file split.  (Bryan Pendelton via cutting)

21. Fix HADOOP-157.  When a task that writes dfs files (e.g., a reduce
    task) failed and was retried, it would fail again and again,
    eventually failing the job.  The problem was that dfs did not yet
    know that the failed task had abandoned the files, and would not
    yet let another task create files with the same names.  Dfs now
    retries when creating a file long enough for locks on abandoned
    files to expire.  (omalley via cutting)

22. Fix HADOOP-150.  Improved task names that include job
    names. (omalley via cutting)

23. Fix HADOOP-162.  Fix ConcurrentModificationException when
    releasing file locks. (omalley via cutting)

24. Fix HADOOP-132.  Initial check-in of new Metrics API, including 
    implementations for writing metric data to a file and for sending
    it to Ganglia.  (David Bowen via cutting)

25. Fix HADOOP-160.  Remove some uneeded synchronization around
    time-consuming operations in the TaskTracker.  (omalley via cutting)

26. Fix HADOOP-166.  RPCs failed when passed subclasses of a declared
    parameter type.  This is fixed by changing ObjectWritable to store
    both the declared type and the instance type for Writables.  Note
    that this incompatibly changes the format of ObjectWritable and
    will render unreadable any ObjectWritables stored in files.
    Nutch only uses ObjectWritable in intermediate files, so this
    should not be a problem for Nutch.  (Stefan & cutting)

27. Fix HADOOP-168.  MapReduce RPC protocol methods should all declare
    IOException, so that timeouts are handled appropriately.
    (omalley via cutting)

28. Fix HADOOP-169.  Don't fail a reduce task if a call to the
    jobtracker to locate map outputs fails.  (omalley via cutting)


Release 0.1.1 - 2006-04-08

 1. Added CHANGES.txt, logging all significant changes to Hadoop.  (cutting)

 2. Fix MapReduceBase.close() to throw IOException, as declared in the
    Closeable interface.  This permits subclasses which override this
    method to throw that exception. (cutting)

 3. Fix HADOOP-117.  Pathnames were mistakenly transposed in
    JobConf.getLocalFile() causing many mapred temporary files to not
    be removed.  (Raghavendra Prabhu via cutting)
 
 4. Fix HADOOP-116. Clean up job submission files when jobs complete.
    (cutting)

 5. Fix HADOOP-125. Fix handling of absolute paths on Windows (cutting)

Release 0.1.0 - 2006-04-01

 1. The first release of Hadoop.

